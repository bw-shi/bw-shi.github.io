---
permalink: /
title: "Bowen Shi"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

About Me
======
I am a research scientist in Fundamental AI Research (FAIR), [Meta AI](https://ai.meta.com/research/). My main research interest is in machine learning for speech and audio. I have worked on a few different topics, including multimodal and multilingual speech, self-supervised speech representation learning and text-to-speech synthesis. My current research is primarily focused on general audio generation. Prior to Meta, I received my Ph.D in Computer Science from [TTI Chicago]() under the advisement of [Dr. Karen Livescu](), where I mainly worked on American Sign Language processing in the wild. Before that, I obtained my M.Sc from [ENSTA Paris]() and [Pierre and Marie Curie University](), and my B.Sc from [Shanghai Jiao Tong University]().

Research Highlights
======
* [AudioBox](https://ai.meta.com/blog/audiobox-generating-audio-voice-natural-language-prompts/): A state-of-the-art unified model capable of generating various audio modalities (e.g., speech, sound effects) with description-based and example-based prompting

* [VoiceBox](https://about.fb.com/news/2023/06/introducing-voicebox-ai-for-speech-generation/): A versatile generative model that can perform a varriety of speech generation tasks (e.g., editing, sampling and styling) through in-context learning

* [CM3Leon](https://ai.meta.com/blog/generative-ai-text-images-cm3leon/): An efficient retrieval-augmented multimodal large language model that does both text-to-image and image-to-text generation

* [MMS](https://about.fb.com/news/2023/05/ai-massively-multilingual-speech-technology/): Massively Multilingual Speech models that expand text-to-speech and speech-to-text technology from around 100 languages to more than 1,100

* [Audio-Visual HuBERT](https://ai.meta.com/blog/ai-that-understands-speech-by-looking-as-well-as-hearing/): The first high-performing self-supervised model for audio-visual speech, achieving state-of-the-art performance on lip-reading and audio-visual speech recognition with much fewer labeled data
